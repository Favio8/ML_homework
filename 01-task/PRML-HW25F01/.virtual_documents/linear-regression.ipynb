


name="张铭扬"
student_id="23009200260"





# 查看个人持久化工作区文件
!ls














import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline


def warm_up_exercise():
    """热身练习"""
    A = None
    # ====================== 你的代码 ==========================
    # 在下面加入你的代码，使程序返回一个 5x5 的单位矩阵
    # =========================================================
    A = np.eye(5) # 直接调用numpy中的eye()方法，来进行快速构造
    return A

# 当你的实现正确时，下面会输出一个单位矩阵：
print(warm_up_exercise())





def plot_data(x, y):
    """绘制给定数据x与y的图像"""
    plt.figure()
    # ====================== 你的代码 ==========================
    # 绘制x与y的图像
    # 使用 matplotlib.pyplt 的命令 plot, xlabel, ylabel 等。
    # 提示：可以使用 'rx' 选项使数据点显示为红色的 "x"，
    #       使用 "markersize=8, markeredgewidth=2" 使标记更大

    # 给制数据

    # 设置y轴标题为 'Profit in $10,000s'
    
    # 设置x轴标题为 'Population of City in 10,000s'
    plt.plot(x,y,'rx',markersize=8,markeredgewidth=2) # 将x和y导入，然后将颜色形状设为red和‘x’，markersize设置标记的大小，markeredgewidth设置标记边缘线的宽度
    plt.ylabel('Profit in $10,000s') # 设置y轴标题
    plt.xlabel('Population of City in 10,000s') # 设置x轴标题
    plt.show() # 将画布进行展示
    # =========================================================
# 让我们测试一下你的实现是否正确
# 从txt中加载数据
print('Plotting Data ...\n')
data = np.loadtxt('./PRML_LR_data.txt', delimiter=',')
x, y = data[:, 0], data[:, 1]

# 绘图
plot_data(x, y)
plt.show()





# Add a column of ones to x
m = len(y)
X = np.ones((m, 2))
X[:, 1] = data[:, 0]

# initialize fitting parameters
theta = np.zeros((2, 1))
print(theta)
print(y.shape)
print(X.shape)
print((X @ theta).shape)
# Some gradient descent settings
iterations = 1500
alpha = 0.01





def compute_cost(X, y, theta):
    """计算线性回归的代价。"""
    m = len(y)
    J = 0.0
    # ====================== 你的代码 ==========================
    # 计算给定 theta 参数下线性回归的代价
    # 请将正确的代价赋值给 J
    predictions = X @ theta  # @进行矩阵乘法来计算模型的预测值 结果为m×2 2×1->m×1
    errors = predictions - y.reshape(-1, 1) # 确保y是列向量 对应相减，得到对应的errors
    J = (1 / (2 * m)) * np.sum(errors ** 2) # 按照公式来对应计算全部偏差的平方和
    # =========================================================
    return J

# compute and display initial cost
# Expected value 32.07
J0 = compute_cost(X, y, theta)
print(J0)





def gradient_descent(X, y, theta, alpha, num_iters):
    """执行梯度下降算法来学习参数 theta。"""
    m = len(y)
    J_history = np.zeros((num_iters,)) # 用来记录每次迭代后计算出来的代价值
    theta_history = [] # 用来记录每一次迭代后的θ值，用于后续的可视化
    for iter in range(num_iters):
        # ====================== 你的代码 ==========================
        # 计算给定 theta 参数下线性回归的梯度，实现梯度下降算法
        theta_history.append(theta.copy().flatten()) # 将当前的θ进行复制并进行拉平然后存入theta_history
        predictions = X @ theta # 通过当前的X与θ来计算得到对应的预测值
        errors = predictions - y.reshape(-1, 1) # 然后进行作差得到对应的errors
        gradient = (1 / m) * (X.T @ errors) # 通过公式推导求得对应梯度
        theta = theta - alpha * gradient # 进行梯度更新
        # =========================================================
        # 将各次迭代后的代价进行记录
        J_history[iter] = compute_cost(X, y, theta)

    return theta, J_history, theta_history

# run gradient descent
# Expected value: theta = [-3.630291, 1.166362]
theta, J_history, _ = gradient_descent(X, y, theta,
                                    alpha, iterations)
print(theta)
print(J_history)


def plot_fit_result(X, y, theta):
    plt.figure(figsize=(10, 6))
    plt.plot(X[:,1], y, 'rx', markersize=8, markeredgewidth=2, label='train data') # 将训练样本画出来

    x_line = np.linspace(X[:, 1].min(), X[:, 1].max(), 100) # 创建100个点 覆盖x的最小值和最大值范围
    y_line = theta[0] + theta[1] * x_line # 通过公式进行对应y值的计算

    plt.plot(x_line, y_line, 'b-', linewidth=2, label='linear regression')
    plt.xlabel('Population of City in 10,000s') # 设置对应的图例
    plt.ylabel('Profit in $10,000s')
    plt.legend()
    plt.grid(True, alpha=0.3) # 设置网格
    plt.show()
    

plot_fit_result(X, y, theta)





def plot_visualize_cost(X, y, theta_best):
    """可视化代价函数"""

    # 生成参数网格
    theta0_vals = np.linspace(-10, 10, 101) # 生成101个θ_0值 从-10到10
    theta1_vals = np.linspace(-1, 4, 101) # 生成101个θ_1值 从-1到4
    t = np.zeros((2, 1)) # 存储对应的theta值
    J_vals = np.zeros((101, 101)) # 存储对应的theta_0和theta_1
    for i in range(101): # 循环式为了遍历101×101个不同的theta组合，并计算每一组参数下面的代价，填入J_vals中
        for j in range(101):
            # =============== 你的代码 ===================
            # 加入代码，计算 J_vals 的值
            t = np.array([[theta0_vals[i]],
                         [theta1_vals[j]]])
            J_vals[j, i] = compute_cost(X, y, t) 
            # ===========================================

    plt.figure()
    plt.contour(theta0_vals, theta1_vals, J_vals,
                levels=np.logspace(-2, 3, 21))
    plt.plot(theta_best[0], theta_best[1], 'rx',
             markersize=8, markeredgewidth=2)
    plt.xlabel(r'$\theta_0$')
    plt.ylabel(r'$\theta_1$')
    plt.title(r'$J(\theta)$')
  

plot_visualize_cost(X, y, theta)
plt.show()





def plot_visual_history(X, y, theta_history):
    plt.figure()
    # =============== 你的代码 ===================
    theta0_vals = np.linspace(-10, 10, 101)
    theta1_vals = np.linspace(-1, 4, 101)
    J_vals = np.zeros((101, 101))
    for i in range(101):
        for j in range(101):
            t = np.array([[theta0_vals[i]], [theta1_vals[j]]])
            J_vals[j, i] = compute_cost(X, y, t)
    
    plt.contour(theta0_vals, theta1_vals, J_vals, levels=np.logspace(-2, 3, 21)) # 设置对数刻度

    theta_history = np.array(theta_history)
    plt.plot(theta_history[:, 0], theta_history[:, 1], 'r.-', label='optimized path')
    plt.plot(theta_history[-1,0], theta_history[-1, 1], 'rx', markersize=8, markeredgewidth=2, label='final theta')
    plt.legend()
    # ===========================================
    plt.xlabel(r'$\theta_0$')
    plt.ylabel(r'$\theta_1$')
    plt.title(r'$J(\theta)$')
    
theta, J_history, theta_history = gradient_descent(X, y, theta, alpha, iterations)
plot_visual_history(X, y, theta_history)
plt.show()



